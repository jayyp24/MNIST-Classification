{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. \n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\anaconda3\\newana\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "That's where we load and preprocess our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "#validation data is 10% of training data\n",
    "\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "#following function converts the image datatype to float and then standardizes it and returns\n",
    "def scale(image, label):\n",
    "  \n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    image /= 255.\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling is performed\n",
    "\n",
    "shuffled_train_vali_data = scaled_train_and_validation_data.shuffle(buffer_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationdata = shuffled_train_vali_data.take(num_validation_samples)\n",
    "traindata= shuffled_train_vali_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = traindata.batch(batch_size)\n",
    "\n",
    "validationdata = validationdata.batch(num_validation_samples)\n",
    "\n",
    "\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validationdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layers = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the following model there are 2 hidden layers of width 50 each. W have used relu and softmax activaton to generate non linear outputs\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "\n",
    "  tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "  tf.keras.layers.Dense(hidden_layers, activation='relu'),\n",
    "  tf.keras.layers.Dense(hidden_layers, activation='relu'),\n",
    "  tf.keras.layers.Dense(output_size, activation='softmax'),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540/540 - 6s - loss: 0.4140 - accuracy: 0.8833 - val_loss: 0.2106 - val_accuracy: 0.9395 - 6s/epoch - 11ms/step\n",
      "Epoch 2/4\n",
      "540/540 - 3s - loss: 0.1754 - accuracy: 0.9483 - val_loss: 0.1540 - val_accuracy: 0.9567 - 3s/epoch - 6ms/step\n",
      "Epoch 3/4\n",
      "540/540 - 3s - loss: 0.1332 - accuracy: 0.9600 - val_loss: 0.1276 - val_accuracy: 0.9643 - 3s/epoch - 6ms/step\n",
      "Epoch 4/4\n",
      "540/540 - 3s - loss: 0.1087 - accuracy: 0.9676 - val_loss: 0.1066 - val_accuracy: 0.9707 - 3s/epoch - 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e27351b220>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numepochs = 4\n",
    "\n",
    "\n",
    "model.fit(traindata, epochs=numepochs, validation_data=(validation_inputs, validation_targets), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 945ms/step - loss: 0.1144 - accuracy: 0.9648\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
